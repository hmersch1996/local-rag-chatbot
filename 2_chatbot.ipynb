{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import llama_index\n",
    "import ollama\n",
    "import os\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "chroma_collection = chroma_init(\"./chroma_db\")\n",
    "\n",
    "storage_context = create_llama_connectors(chroma_collection)\n",
    "\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "\n",
    "llm = Ollama(model=\"deepseek-r1:latest\", request_timeout=1200.0, device='cuda')\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-base-en-v1.5\", device=\"cuda\")\n",
    "\n",
    "index = VectorStoreIndex.from_vector_store(\n",
    "    vector_store = vector_store,\n",
    "    storage_context = storage_context,\n",
    "    embed_model=embed_model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = index.as_chat_engine(\n",
    "    llm=llm,\n",
    "    similarity_top_k=3,\n",
    "    streaming=True,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Running step afc2b29f-5d3d-475b-b3a9-4fa753aba7e1. Step input: Hola\n",
      "\u001b[1;3;38;5;200mThought: The user has greeted me with \"Hola,\" which is Spanish for \"Hello.\" Since my instructions are primarily in English but I can process some Spanish queries, I might need to clarify the language they want their response in. However, as a general approach, responding in English unless specified otherwise makes sense.\n",
      "Answer: Sure! How can I assist you today?\n",
      "\u001b[0m"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AgentChatResponse(response='Sure! How can I assist you today?', sources=[], source_nodes=[], is_dummy_stream=False, metadata=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.chat(\"Hola\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrapper_chat_history(memory):\n",
    "    chat_history = []\n",
    "    for m in memory: \n",
    "        if m.role in ['user','assistant'] and m.content is not None:\n",
    "            chat_history.append(m.content)\n",
    "    return chat_history\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hola', 'Sure! How can I assist you today?']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapper_chat_history(chat.chat_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def converse(message, chat_history):\n",
    "    response = chat.chat(message)\n",
    "    chat_history = wrapper_chat_history(chat.chat_history)\n",
    "\n",
    "    return response.response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hm/anaconda3/envs/local-rag/lib/python3.11/site-packages/gradio/components/chatbot.py:288: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Running step f39609de-ff02-416d-a4ee-bb6aab8f7bc0. Step input: hola, soy hans, tu companhero para desarmar bombas, me gusta las naranjas\n",
      "\u001b[1;3;38;5;200mThought: (Implicit) I can answer without any more tools!\n",
      "Answer: <think>\n",
      "Okay, so I'm trying to figure out what Hans wants. He greeted me in Spanish with \"hola, soy hans, tucompanhero para desarmar bombas, me gusta las naranjas.\" Translating that, it means he's my partner to defuse bombs and likes oranges.\n",
      "\n",
      "Hmm, he mentioned being a partner for bomb disposal. That makes me think he might be involved in some kind of security or protective work. Maybe he needs assistance with something related to that? He also said he likes oranges, so perhaps that's just a casual statement about his preferences.\n",
      "\n",
      "I'm not sure if I can provide help directly since my tools are set up for certain tasks like answering questions or providing summaries. But maybe there's a way to infer what he needs without direct communication. If someone is involved in bomb disposal, they might need information on how to safely handle explosives, which could involve looking up specific procedures or guidelines.\n",
      "\n",
      "I could use the query_engine_tool to get information on bomb disposal techniques or safety measures. That might help him do his job better. Alternatively, if he's talking about something else, like a game or a story involving oranges, I can assist with that too.\n",
      "\n",
      "So, maybe I should ask for more context or check if there's a specific question related to bomb disposal or oranges. Without more details, it's hard to know exactly what he needs help with.\n",
      "</think>\n",
      "\n",
      "I'm sorry, but without additional context, I can't provide specific assistance regarding bomb disposal or his interest in oranges. If you have any questions or need information on these topics, feel free to ask!\n",
      "\u001b[0m> Running step 7904406a-bcf7-407b-95e1-17d1570e332a. Step input: hola, soy hans, tu companhero para desarmar bombas, me gusta las naranjas\n",
      "\u001b[1;3;38;5;200mThought: (Implicit) I can answer without any more tools!\n",
      "Answer: <think>\n",
      "Okay, so Hans greeted me and mentioned he's my partner for defusing bombs and likes oranges. He seems serious about his work, which could mean he needs help with bomb disposal techniques or safety information.\n",
      "\n",
      "I don't have tools to assist directly in that field, but I can offer general knowledge on bomb safety or ask if there's a specific question related to oranges. Let me know how I can help!\n",
      "</think>\n",
      "\n",
      "It seems like Hans is involved in bomb-related work and mentions liking oranges. If he needs assistance with information on bomb disposal techniques, let me know!\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "import gradio as gr \n",
    "\n",
    "try:\n",
    "    demo.close()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "demo = gr.ChatInterface(fn=converse)\n",
    "\n",
    "demo.launch(share=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local-rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
