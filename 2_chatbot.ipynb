{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import llama_index\n",
    "import ollama\n",
    "import os\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "chroma_collection = chroma_init(\"./chroma_db\")\n",
    "\n",
    "storage_context = create_llama_connectors(chroma_collection)\n",
    "\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "\n",
    "llm = Ollama(model=\"deepseek-r1:latest\", request_timeout=1200.0, device='cuda')\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-base-en-v1.5\", device=\"cuda\")\n",
    "\n",
    "index = VectorStoreIndex.from_vector_store(\n",
    "    vector_store = vector_store,\n",
    "    storage_context = storage_context,\n",
    "    embed_model=embed_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = (\n",
    "\"\"\" \n",
    "Eres un asistente para desactivacion de bombas, tu companhero, el operario encargado de desarmar la bomba, va a darte\n",
    "detalles de la bomba que va a desarmar. Tu debes leer el manual e informar al operario que debe hacer para desactivarla\n",
    "y evitar que la bomba explote.\n",
    "- Debes responder en espanhol\n",
    "- Debes ser claro y conciso\n",
    "- Debes ser el apoyo emocional de tu companhero, el puede estar muy estresado al estar manipulando la bomba\n",
    "- Debes proporcionar respuestas rapidamente\n",
    "\n",
    "Manual de desactivacion de bombas:\n",
    "{context_str}\n",
    "\n",
    "Comentario del operario:\n",
    "{query_str}\n",
    "\n",
    "Responde apropiadamente, sin cometer errores, la vida de las personas esta en tus manos\n",
    "\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "qa_template = PromptTemplate(template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = index.as_chat_engine(\n",
    "    llm=llm,\n",
    "    similarity_top_k=3,\n",
    "    streaming=True,\n",
    "    text_qa_template=qa_template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat.chat(\"Hola\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrapper_chat_history(memory):\n",
    "    chat_history = []\n",
    "    for m in memory: \n",
    "        if m.role in ['user','assistant'] and m.content is not None:\n",
    "            chat_history.append(m.content)\n",
    "    return chat_history\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapper_chat_history(chat.chat_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def converse(message, chat_history):\n",
    "    response = chat.chat(message)\n",
    "    chat_history = wrapper_chat_history(chat.chat_history)\n",
    "\n",
    "    return response.response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat.chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr \n",
    "\n",
    "try:\n",
    "    demo.close()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "demo = gr.ChatInterface(fn=converse)\n",
    "\n",
    "demo.launch(share=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VERSION FINAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hm/anaconda3/envs/local-rag/lib/python3.11/site-packages/gradio/components/chatbot.py:288: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import llama_index\n",
    "import ollama\n",
    "import os\n",
    "import gradio as gr\n",
    "from utils import *\n",
    "\n",
    "# Inicialización de la base de datos Chroma\n",
    "chroma_collection = chroma_init(\"./chroma_db\")\n",
    "storage_context = create_llama_connectors(chroma_collection)\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "\n",
    "# Modelo LLM y embeddings\n",
    "llm = Ollama(model=\"deepseek-r1:latest\", request_timeout=1200.0, device='cuda')\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-base-en-v1.5\", device=\"cuda\")\n",
    "\n",
    "# Creación del índice con VectorStoreIndex\n",
    "index = VectorStoreIndex.from_vector_store(\n",
    "    vector_store=vector_store,\n",
    "    storage_context=storage_context,\n",
    "    embed_model=embed_model\n",
    ")\n",
    "\n",
    "# Plantilla del chatbot\n",
    "template = (\n",
    "\"\"\" \n",
    "Eres un asistente para desactivación de bombas. El operario encargado de desarmar la bomba te proporcionará detalles \n",
    "y debes guiarlo con precisión según el manual. \n",
    "\n",
    "- Debes responder en español.\n",
    "- Debes ser claro y conciso.\n",
    "- Debes ser el apoyo emocional del operario, quien puede estar estresado.\n",
    "- Debes proporcionar respuestas rápidamente.\n",
    "\n",
    "Manual de desactivación de bombas:\n",
    "{context_str}\n",
    "\n",
    "Comentario del operario:\n",
    "{query_str}\n",
    "\n",
    "Responde apropiadamente, sin cometer errores, la vida de las personas está en tus manos.\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "qa_template = PromptTemplate(template)\n",
    "\n",
    "# Creación del motor de chat\n",
    "chat = index.as_chat_engine(\n",
    "    llm=llm,\n",
    "    similarity_top_k=3,\n",
    "    streaming=True,\n",
    "    text_qa_template=qa_template\n",
    ")\n",
    "\n",
    "# Función para manejar el historial de chat\n",
    "def wrapper_chat_history(memory):\n",
    "    chat_history = []\n",
    "    for m in memory:\n",
    "        if m.role in ['user', 'assistant'] and m.content is not None:\n",
    "            chat_history.append(m.content)\n",
    "    return chat_history\n",
    "\n",
    "# Función principal para interactuar con el chatbot\n",
    "def converse(message, chat_history):\n",
    "    response = chat.chat(message)  # Genera la respuesta con el motor de chat\n",
    "    chat_history.append((\"Usuario\", message))  # Agrega el mensaje del usuario\n",
    "    chat_history.append((\"Asistente\", response.response))  # Agrega la respuesta del bot\n",
    "\n",
    "    # Retornar solo el mensaje como string (evita tuplas)\n",
    "    return response.response  # Se devuelve solo la respuesta como string\n",
    "\n",
    "\n",
    "# Intentar cerrar la interfaz si ya está activa\n",
    "try:\n",
    "    demo.close()\n",
    "except NameError:\n",
    "    pass  # No hacer nada si demo no está definido\n",
    "\n",
    "# Creación de la interfaz de Gradio\n",
    "demo = gr.ChatInterface(fn=converse, title=\"Asistente para Desactivación de Bombas\")\n",
    "demo.launch(share=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local-rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
